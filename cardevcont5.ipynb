{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOn3ijb/N9rT/f302vhW1rt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a07a1c1c51c343a38ff19cfcd2f17091": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_437a01746d4143c182bc2e6fa4d4cdf8",
              "IPY_MODEL_3633fc7da80a42ffaec02fb9c3aac85c",
              "IPY_MODEL_f6259d2cdab24754b7de1de191549d35"
            ],
            "layout": "IPY_MODEL_b9734e54ca8c4c3284927acaa0b18ede"
          }
        },
        "437a01746d4143c182bc2e6fa4d4cdf8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a78508e33afa4dccbf6f2341b6377cbd",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_6106c691fa844d25aa23fd23d7cf8c6e",
            "value": "open_clip_model.safetensors:â€‡100%"
          }
        },
        "3633fc7da80a42ffaec02fb9c3aac85c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f8ef0b5c83046cba3d834f717e5b7a9",
            "max": 605143284,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_23e7868288214412a5b4ce5202065915",
            "value": 605143284
          }
        },
        "f6259d2cdab24754b7de1de191549d35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0023c0b1b3a7427092ce7e7e3935789a",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_3d524b3c2c90471ab549a2ca1bf6cd05",
            "value": "â€‡605M/605Mâ€‡[00:05&lt;00:00,â€‡141MB/s]"
          }
        },
        "b9734e54ca8c4c3284927acaa0b18ede": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a78508e33afa4dccbf6f2341b6377cbd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6106c691fa844d25aa23fd23d7cf8c6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3f8ef0b5c83046cba3d834f717e5b7a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23e7868288214412a5b4ce5202065915": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0023c0b1b3a7427092ce7e7e3935789a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d524b3c2c90471ab549a2ca1bf6cd05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ysnkchk0727-rgb/-/blob/main/cardevcont5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdSzldgZVOK3",
        "outputId": "52e8f5e3-2136-4adb-8dab-5207d2ccbf74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# ğŸ”¹ ë©€í‹°ëª¨ë‹¬ LLM ê³„ì—´ì¸ CLIP ì‚¬ìš©ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
        "!pip install open_clip_torch --quiet\n",
        "!pip install pandas --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# ğŸ”¹ Google Driveì— ìˆëŠ” ì••ì¶• íŒŒì¼ ê²½ë¡œ (ë„¤ê°€ ì‚¬ìš©í•˜ë˜ ê²½ë¡œ ê·¸ëŒ€ë¡œ)\n",
        "zip_path = \"/content/drive/MyDrive/train_data.zip\"\n",
        "\n",
        "# ğŸ”¹ ì••ì¶•ì„ í’€ ìœ„ì¹˜\n",
        "extract_path = \"/content/train_data\"\n",
        "\n",
        "# ì´ë¯¸ í’€ë ¤ ìˆìœ¼ë©´ ë‹¤ì‹œ ì•ˆ í’€ì–´ë„ ë¨\n",
        "if not os.path.exists(extract_path) or len(os.listdir(extract_path)) == 0:\n",
        "    os.makedirs(extract_path, exist_ok=True)\n",
        "    with zipfile.ZipFile(zip_path, 'r') as z:\n",
        "        z.extractall(extract_path)\n",
        "    print(\"ì••ì¶• í•´ì œ ì™„ë£Œ:\", extract_path)\n",
        "else:\n",
        "    print(\"ì´ë¯¸ ì••ì¶•ì´ í’€ë ¤ ìˆìŠµë‹ˆë‹¤:\", extract_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilqNfwRFVu2p",
        "outputId": "a11472e6-c43c-4c2d-9278-ebe234b7935c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "ì••ì¶• í•´ì œ ì™„ë£Œ: /content/train_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "\n",
        "import open_clip  # ë°©ê¸ˆ ì„¤ì¹˜í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
        "\n",
        "# ğŸ”¹ ë””ë°”ì´ìŠ¤ ì„¤ì •\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"ì‚¬ìš© ë””ë°”ì´ìŠ¤:\", device)\n",
        "\n",
        "# ğŸ”¹ CLIP ëª¨ë¸(ë¹„ì „-ì–¸ì–´ ë©€í‹°ëª¨ë‹¬ LLM) ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "# ViT-B-32 ë²„ì „: ì†ë„/ë©”ëª¨ë¦¬ ì ë‹¹í•œ í¸\n",
        "model, _, preprocess = open_clip.create_model_and_transforms(\n",
        "    'ViT-B-32',\n",
        "    pretrained='openai'\n",
        ")\n",
        "tokenizer = open_clip.get_tokenizer('ViT-B-32')\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# ğŸ”¹ ImageFolderë¡œ ë°ì´í„°ì…‹ ë¡œë“œ (í´ë”ëª…ì´ ë¼ë²¨)\n",
        "data_dir = \"/content/train_data\"\n",
        "full_dataset = datasets.ImageFolder(data_dir, transform=preprocess)\n",
        "\n",
        "print(\"í´ë˜ìŠ¤(í´ë”) ì´ë¦„:\", full_dataset.classes)\n",
        "num_classes = len(full_dataset.classes)\n",
        "print(\"í´ë˜ìŠ¤ ê°œìˆ˜:\", num_classes)\n",
        "print(\"ì´ ì´ë¯¸ì§€ ê°œìˆ˜:\", len(full_dataset))\n",
        "\n",
        "# ğŸ”¹ í•œ ë²ˆë§Œ ì“¸ \"ê³ ì •ëœ train/val ë¶„í• \" ë§Œë“¤ê¸°\n",
        "num_samples = len(full_dataset)\n",
        "indices = list(range(num_samples))\n",
        "\n",
        "# seed ê³ ì • â†’ í•­ìƒ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ì„ì„\n",
        "random.seed(42)\n",
        "random.shuffle(indices)\n",
        "\n",
        "train_ratio = 0.8\n",
        "train_size = int(num_samples * train_ratio)\n",
        "train_indices = indices[:train_size]\n",
        "val_indices = indices[train_size:]\n",
        "\n",
        "print(\"train ê°œìˆ˜:\", len(train_indices))\n",
        "print(\"val ê°œìˆ˜:\", len(val_indices))\n",
        "\n",
        "# ğŸ”¹ Subsetìœ¼ë¡œ train/val ë‚˜ëˆ„ê¸°\n",
        "train_dataset = Subset(full_dataset, train_indices)\n",
        "val_dataset = Subset(full_dataset, val_indices)\n",
        "\n",
        "# DataLoader (CLIP zero-shotì´ë‹ˆê¹Œ shuffleì€ êµ³ì´ í•„ìš” ì—†ìŒ)\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=False)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304,
          "referenced_widgets": [
            "a07a1c1c51c343a38ff19cfcd2f17091",
            "437a01746d4143c182bc2e6fa4d4cdf8",
            "3633fc7da80a42ffaec02fb9c3aac85c",
            "f6259d2cdab24754b7de1de191549d35",
            "b9734e54ca8c4c3284927acaa0b18ede",
            "a78508e33afa4dccbf6f2341b6377cbd",
            "6106c691fa844d25aa23fd23d7cf8c6e",
            "3f8ef0b5c83046cba3d834f717e5b7a9",
            "23e7868288214412a5b4ce5202065915",
            "0023c0b1b3a7427092ce7e7e3935789a",
            "3d524b3c2c90471ab549a2ca1bf6cd05"
          ]
        },
        "id": "CQdNvnjuV-Fr",
        "outputId": "7a73bef4-523f-4fa3-ed89-c172e5ad0edb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì‚¬ìš© ë””ë°”ì´ìŠ¤: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "open_clip_model.safetensors:   0%|          | 0.00/605M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a07a1c1c51c343a38ff19cfcd2f17091"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/open_clip/factory.py:450: UserWarning: QuickGELU mismatch between final model config (quick_gelu=False) and pretrained tag 'openai' (quick_gelu=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "í´ë˜ìŠ¤(í´ë”) ì´ë¦„: ['front', 'front_left', 'front_right', 'inside', 'left', 'rear', 'rear_left', 'rear_right', 'right', 'tire']\n",
            "í´ë˜ìŠ¤ ê°œìˆ˜: 10\n",
            "ì´ ì´ë¯¸ì§€ ê°œìˆ˜: 100\n",
            "train ê°œìˆ˜: 80\n",
            "val ê°œìˆ˜: 20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ğŸ”¹ í´ë” ì´ë¦„(ë¼ë²¨) ìˆœì„œ\n",
        "class_names = full_dataset.classes\n",
        "print(\"class_names:\", class_names)\n",
        "\n",
        "# ğŸ”¹ ê° í´ë˜ìŠ¤ì— ëŒ€í•œ í…ìŠ¤íŠ¸ ì„¤ëª…(í”„ë¡¬í”„íŠ¸) ì •ì˜\n",
        "prompt_templates = {\n",
        "    \"front\":       \"a photo of the front of a car\",\n",
        "    \"rear\":        \"a photo of the rear of a car\",\n",
        "    \"left\":        \"a photo of the left side of a car\",\n",
        "    \"right\":       \"a photo of the right side of a car\",\n",
        "    \"front_left\":  \"a front-left diagonal view of a car\",\n",
        "    \"front_right\": \"a front-right diagonal view of a car\",\n",
        "    \"rear_left\":   \"a rear-left diagonal view of a car\",\n",
        "    \"rear_right\":  \"a rear-right diagonal view of a car\",\n",
        "    \"inside\":      \"an interior photo of a car showing seats or dashboard\",\n",
        "    \"tire\":        \"a close-up photo of a car tire or wheel\"\n",
        "}\n",
        "\n",
        "# ğŸ”¹ ImageFolderê°€ ê°€ì§„ class ìˆœì„œì— ë§ì¶°ì„œ ë¬¸ì¥ ë°°ì—´ ë§Œë“¤ê¸°\n",
        "texts = [prompt_templates[c] for c in class_names]\n",
        "print(\"ì‚¬ìš©í•  í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸:\")\n",
        "for t in texts:\n",
        "    print(\" -\", t)\n",
        "\n",
        "# ğŸ”¹ í…ìŠ¤íŠ¸ë¥¼ CLIP text encoderë¡œ ì„ë² ë”©\n",
        "text_tokens = tokenizer(texts).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    text_features = model.encode_text(text_tokens)\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True)  # ì •ê·œí™”"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kWt-zSAqWHJW",
        "outputId": "96aca689-10d0-4522-98d7-f36752319dcf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class_names: ['front', 'front_left', 'front_right', 'inside', 'left', 'rear', 'rear_left', 'rear_right', 'right', 'tire']\n",
            "ì‚¬ìš©í•  í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸:\n",
            " - a photo of the front of a car\n",
            " - a front-left diagonal view of a car\n",
            " - a front-right diagonal view of a car\n",
            " - an interior photo of a car showing seats or dashboard\n",
            " - a photo of the left side of a car\n",
            " - a photo of the rear of a car\n",
            " - a rear-left diagonal view of a car\n",
            " - a rear-right diagonal view of a car\n",
            " - a photo of the right side of a car\n",
            " - a close-up photo of a car tire or wheel\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def evaluate_clip_zero_shot(model, data_loader, text_features, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(data_loader):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # ì´ë¯¸ì§€ ì„ë² ë”©\n",
        "            image_features = model.encode_image(images)\n",
        "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "            # ì´ë¯¸ì§€ vs í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ (ë‚´ì )\n",
        "            logits = 100.0 * image_features @ text_features.T  # [batch, 10]\n",
        "            preds = torch.argmax(logits, dim=-1)\n",
        "\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    acc = correct / total * 100.0\n",
        "    return acc\n",
        "\n",
        "# ğŸ”¹ train/val ì •í™•ë„ ê³„ì‚°\n",
        "train_acc = evaluate_clip_zero_shot(model, train_loader, text_features, device)\n",
        "val_acc = evaluate_clip_zero_shot(model, val_loader, text_features, device)\n",
        "\n",
        "print(f\"CLIP zero-shot Train Accuracy: {train_acc:.2f}%\")\n",
        "print(f\"CLIP zero-shot Val   Accuracy: {val_acc:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9Hm5P3kWI_9",
        "outputId": "224a4bc0-c0ef-44c5-e1ab-636aba7c59ac"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:14<00:00,  2.98s/it]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:03<00:00,  1.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP zero-shot Train Accuracy: 40.00%\n",
            "CLIP zero-shot Val   Accuracy: 30.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import pandas as pd\n",
        "\n",
        "rows = []\n",
        "\n",
        "# ğŸ”¹ full_dataset.samples: (ì´ë¯¸ì§€ ê²½ë¡œ, ì •ë‹µ ë¼ë²¨ ì¸ë±ìŠ¤) ë¦¬ìŠ¤íŠ¸\n",
        "for img_path, _ in tqdm(full_dataset.samples):\n",
        "    # ì´ë¯¸ì§€ ë¡œë“œ\n",
        "    img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "    # CLIP ì „ì²˜ë¦¬ ì ìš©\n",
        "    img_tensor = preprocess(img).unsqueeze(0).to(device)  # [1, C, H, W]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        img_feat = model.encode_image(img_tensor)\n",
        "        img_feat /= img_feat.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        logits = 100.0 * img_feat @ text_features.T\n",
        "        pred_idx = int(torch.argmax(logits, dim=-1).item())\n",
        "\n",
        "    pred_label = class_names[pred_idx]\n",
        "    filename = os.path.basename(img_path)\n",
        "\n",
        "    rows.append({\"filename\": filename, \"prediction\": pred_label})\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "save_path = \"/content/clip_result.csv\"\n",
        "df.to_csv(save_path, index=False)\n",
        "\n",
        "print(\"ì˜ˆì¸¡ ê²°ê³¼ CSV ì €ì¥ ì™„ë£Œ:\", save_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MO7cgjETWNpG",
        "outputId": "6a1b6943-b575-458d-fade-81220cce0eeb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:25<00:00,  3.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì˜ˆì¸¡ ê²°ê³¼ CSV ì €ì¥ ì™„ë£Œ: /content/clip_result.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "# í˜¹ì‹œ ëª¨ë¥¼ ìƒí™© ëŒ€ë¹„í•´ì„œ ë‹¤ì‹œ í•œ ë²ˆ í™•ì¸\n",
        "print(\"train ê°œìˆ˜:\", len(train_dataset))\n",
        "print(\"val ê°œìˆ˜:\", len(val_dataset))\n",
        "\n",
        "def extract_features(dataset, model, device):\n",
        "    \"\"\"\n",
        "    dataset: Subset(full_dataset, indices)\n",
        "    model: CLIP ëª¨ë¸ (image encoder ì‚¬ìš©)\n",
        "    return: (features, labels)\n",
        "    \"\"\"\n",
        "    loader = DataLoader(dataset, batch_size=16, shuffle=False)\n",
        "    all_feats = []\n",
        "    all_labels = []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in tqdm(loader, desc=\"Extracting features\"):\n",
        "            imgs = imgs.to(device)\n",
        "            # CLIP ì´ë¯¸ì§€ ì„ë² ë”©\n",
        "            feats = model.encode_image(imgs)\n",
        "            # ì •ê·œí™” (cosine similarity ê¸°ë°˜ì´ë¼ ë³´í†µ ì´ë ‡ê²Œ ë§ì¶°ì¤Œ)\n",
        "            feats = feats / feats.norm(dim=-1, keepdim=True)\n",
        "\n",
        "            all_feats.append(feats.cpu())\n",
        "            all_labels.append(labels)\n",
        "\n",
        "    all_feats = torch.cat(all_feats, dim=0)   # [N, D]\n",
        "    all_labels = torch.cat(all_labels, dim=0) # [N]\n",
        "    return all_feats, all_labels\n",
        "\n",
        "train_feats, train_labels = extract_features(train_dataset, model, device)\n",
        "val_feats, val_labels = extract_features(val_dataset, model, device)\n",
        "\n",
        "print(\"train_feats shape:\", train_feats.shape)\n",
        "print(\"val_feats shape:\", val_feats.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjpUtAxHZjwu",
        "outputId": "9bbc7e81-6dcb-4a5c-c5fa-ac6462ef8796"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train ê°œìˆ˜: 80\n",
            "val ê°œìˆ˜: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:19<00:00,  3.98s/it]\n",
            "Extracting features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:03<00:00,  1.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_feats shape: torch.Size([80, 512])\n",
            "val_feats shape: torch.Size([20, 512])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# ğŸ”¹ feature ì°¨ì› & í´ë˜ìŠ¤ ìˆ˜\n",
        "feature_dim = train_feats.shape[1]\n",
        "num_classes = len(full_dataset.classes)\n",
        "print(\"feature_dim:\", feature_dim, \"num_classes:\", num_classes)\n",
        "\n",
        "# ğŸ”¹ ê°„ë‹¨í•œ Linear ë¶„ë¥˜ê¸° (softmax ë¶„ë¥˜)\n",
        "classifier = nn.Linear(feature_dim, num_classes).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(classifier.parameters(), lr=1e-2, weight_decay=1e-4)\n",
        "\n",
        "# ğŸ”¹ feature/labelë¡œ êµ¬ì„±ëœ Dataset & DataLoader\n",
        "train_ds_feat = TensorDataset(train_feats, train_labels)\n",
        "val_ds_feat = TensorDataset(val_feats, val_labels)\n",
        "\n",
        "train_loader_feat = DataLoader(train_ds_feat, batch_size=16, shuffle=True)\n",
        "val_loader_feat = DataLoader(val_ds_feat, batch_size=16, shuffle=False)\n",
        "\n",
        "def train_classifier_one_epoch(classifier, loader, optimizer, criterion, device):\n",
        "    classifier.train()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for feats, labels in loader:\n",
        "        feats = feats.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = classifier(feats)  # [B, num_classes]\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * feats.size(0)\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += feats.size(0)\n",
        "\n",
        "    return total_loss / total, correct / total * 100.0\n",
        "\n",
        "def eval_classifier(classifier, loader, criterion, device):\n",
        "    classifier.eval()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for feats, labels in loader:\n",
        "            feats = feats.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = classifier(feats)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            total_loss += loss.item() * feats.size(0)\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += feats.size(0)\n",
        "\n",
        "    return total_loss / total, correct / total * 100.0\n",
        "\n",
        "EPOCHS = 50\n",
        "best_val_acc = 0.0\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    train_loss, train_acc = train_classifier_one_epoch(\n",
        "        classifier, train_loader_feat, optimizer, criterion, device\n",
        "    )\n",
        "    val_loss, val_acc = eval_classifier(\n",
        "        classifier, val_loader_feat, criterion, device\n",
        "    )\n",
        "\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        best_state = classifier.state_dict()\n",
        "\n",
        "    print(f\"[Epoch {epoch:02d}] \"\n",
        "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% | \"\n",
        "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "print(\"\\nìµœê³  ê²€ì¦ ì •í™•ë„ (CLIP feature + Linear classifier): \"\n",
        "      f\"{best_val_acc:.2f}%\")\n",
        "\n",
        "# ğŸ”¹ ìµœê³  ì„±ëŠ¥ì¼ ë•Œì˜ ê°€ì¤‘ì¹˜ë¡œ ë˜ëŒë¦¬ê¸°\n",
        "classifier.load_state_dict(best_state)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsmbRm50ZnBc",
        "outputId": "0a2bfbf6-fd19-4092-e205-ac479833d679"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "feature_dim: 512 num_classes: 10\n",
            "[Epoch 01] Train Loss: 2.2993, Train Acc: 12.50% | Val Loss: 2.2766, Val Acc: 15.00%\n",
            "[Epoch 02] Train Loss: 2.1509, Train Acc: 47.50% | Val Loss: 2.2362, Val Acc: 30.00%\n",
            "[Epoch 03] Train Loss: 2.0421, Train Acc: 56.25% | Val Loss: 2.2092, Val Acc: 30.00%\n",
            "[Epoch 04] Train Loss: 1.9286, Train Acc: 62.50% | Val Loss: 2.1598, Val Acc: 30.00%\n",
            "[Epoch 05] Train Loss: 1.8305, Train Acc: 70.00% | Val Loss: 2.1027, Val Acc: 35.00%\n",
            "[Epoch 06] Train Loss: 1.7390, Train Acc: 72.50% | Val Loss: 2.0418, Val Acc: 40.00%\n",
            "[Epoch 07] Train Loss: 1.6483, Train Acc: 80.00% | Val Loss: 1.9810, Val Acc: 40.00%\n",
            "[Epoch 08] Train Loss: 1.5602, Train Acc: 81.25% | Val Loss: 1.9314, Val Acc: 40.00%\n",
            "[Epoch 09] Train Loss: 1.4789, Train Acc: 83.75% | Val Loss: 1.8933, Val Acc: 40.00%\n",
            "[Epoch 10] Train Loss: 1.4067, Train Acc: 86.25% | Val Loss: 1.8519, Val Acc: 40.00%\n",
            "[Epoch 11] Train Loss: 1.3369, Train Acc: 87.50% | Val Loss: 1.8131, Val Acc: 40.00%\n",
            "[Epoch 12] Train Loss: 1.2744, Train Acc: 91.25% | Val Loss: 1.7753, Val Acc: 40.00%\n",
            "[Epoch 13] Train Loss: 1.2122, Train Acc: 91.25% | Val Loss: 1.7456, Val Acc: 40.00%\n",
            "[Epoch 14] Train Loss: 1.1580, Train Acc: 92.50% | Val Loss: 1.7197, Val Acc: 40.00%\n",
            "[Epoch 15] Train Loss: 1.1067, Train Acc: 95.00% | Val Loss: 1.6972, Val Acc: 40.00%\n",
            "[Epoch 16] Train Loss: 1.0560, Train Acc: 95.00% | Val Loss: 1.6664, Val Acc: 45.00%\n",
            "[Epoch 17] Train Loss: 1.0137, Train Acc: 95.00% | Val Loss: 1.6428, Val Acc: 45.00%\n",
            "[Epoch 18] Train Loss: 0.9708, Train Acc: 97.50% | Val Loss: 1.6207, Val Acc: 40.00%\n",
            "[Epoch 19] Train Loss: 0.9307, Train Acc: 97.50% | Val Loss: 1.6081, Val Acc: 40.00%\n",
            "[Epoch 20] Train Loss: 0.8929, Train Acc: 97.50% | Val Loss: 1.5983, Val Acc: 40.00%\n",
            "[Epoch 21] Train Loss: 0.8593, Train Acc: 98.75% | Val Loss: 1.5881, Val Acc: 40.00%\n",
            "[Epoch 22] Train Loss: 0.8267, Train Acc: 98.75% | Val Loss: 1.5743, Val Acc: 40.00%\n",
            "[Epoch 23] Train Loss: 0.7950, Train Acc: 98.75% | Val Loss: 1.5623, Val Acc: 45.00%\n",
            "[Epoch 24] Train Loss: 0.7687, Train Acc: 98.75% | Val Loss: 1.5433, Val Acc: 45.00%\n",
            "[Epoch 25] Train Loss: 0.7405, Train Acc: 98.75% | Val Loss: 1.5353, Val Acc: 45.00%\n",
            "[Epoch 26] Train Loss: 0.7134, Train Acc: 98.75% | Val Loss: 1.5209, Val Acc: 50.00%\n",
            "[Epoch 27] Train Loss: 0.6910, Train Acc: 98.75% | Val Loss: 1.5168, Val Acc: 40.00%\n",
            "[Epoch 28] Train Loss: 0.6672, Train Acc: 98.75% | Val Loss: 1.5059, Val Acc: 55.00%\n",
            "[Epoch 29] Train Loss: 0.6456, Train Acc: 98.75% | Val Loss: 1.5059, Val Acc: 50.00%\n",
            "[Epoch 30] Train Loss: 0.6233, Train Acc: 98.75% | Val Loss: 1.4983, Val Acc: 50.00%\n",
            "[Epoch 31] Train Loss: 0.6050, Train Acc: 98.75% | Val Loss: 1.4886, Val Acc: 50.00%\n",
            "[Epoch 32] Train Loss: 0.5866, Train Acc: 98.75% | Val Loss: 1.4873, Val Acc: 50.00%\n",
            "[Epoch 33] Train Loss: 0.5677, Train Acc: 100.00% | Val Loss: 1.4806, Val Acc: 50.00%\n",
            "[Epoch 34] Train Loss: 0.5508, Train Acc: 100.00% | Val Loss: 1.4685, Val Acc: 50.00%\n",
            "[Epoch 35] Train Loss: 0.5360, Train Acc: 100.00% | Val Loss: 1.4560, Val Acc: 55.00%\n",
            "[Epoch 36] Train Loss: 0.5197, Train Acc: 100.00% | Val Loss: 1.4533, Val Acc: 55.00%\n",
            "[Epoch 37] Train Loss: 0.5043, Train Acc: 100.00% | Val Loss: 1.4506, Val Acc: 55.00%\n",
            "[Epoch 38] Train Loss: 0.4916, Train Acc: 100.00% | Val Loss: 1.4517, Val Acc: 55.00%\n",
            "[Epoch 39] Train Loss: 0.4776, Train Acc: 100.00% | Val Loss: 1.4566, Val Acc: 55.00%\n",
            "[Epoch 40] Train Loss: 0.4650, Train Acc: 100.00% | Val Loss: 1.4611, Val Acc: 55.00%\n",
            "[Epoch 41] Train Loss: 0.4528, Train Acc: 100.00% | Val Loss: 1.4587, Val Acc: 55.00%\n",
            "[Epoch 42] Train Loss: 0.4410, Train Acc: 100.00% | Val Loss: 1.4520, Val Acc: 55.00%\n",
            "[Epoch 43] Train Loss: 0.4304, Train Acc: 100.00% | Val Loss: 1.4420, Val Acc: 55.00%\n",
            "[Epoch 44] Train Loss: 0.4198, Train Acc: 100.00% | Val Loss: 1.4283, Val Acc: 55.00%\n",
            "[Epoch 45] Train Loss: 0.4095, Train Acc: 100.00% | Val Loss: 1.4224, Val Acc: 55.00%\n",
            "[Epoch 46] Train Loss: 0.3990, Train Acc: 100.00% | Val Loss: 1.4203, Val Acc: 55.00%\n",
            "[Epoch 47] Train Loss: 0.3899, Train Acc: 100.00% | Val Loss: 1.4251, Val Acc: 55.00%\n",
            "[Epoch 48] Train Loss: 0.3823, Train Acc: 100.00% | Val Loss: 1.4287, Val Acc: 55.00%\n",
            "[Epoch 49] Train Loss: 0.3723, Train Acc: 100.00% | Val Loss: 1.4226, Val Acc: 55.00%\n",
            "[Epoch 50] Train Loss: 0.3646, Train Acc: 100.00% | Val Loss: 1.4268, Val Acc: 55.00%\n",
            "\n",
            "ìµœê³  ê²€ì¦ ì •í™•ë„ (CLIP feature + Linear classifier): 55.00%\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import pandas as pd\n",
        "\n",
        "rows = []\n",
        "\n",
        "# ğŸ”¹ full_dataset ìˆœì„œëŒ€ë¡œ feature ë½‘ê³  â†’ classifier í†µê³¼\n",
        "full_loader = DataLoader(full_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "model.eval()\n",
        "classifier.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in tqdm(full_loader, desc=\"Predicting all images\"):\n",
        "        imgs = imgs.to(device)\n",
        "        # CLIP ì´ë¯¸ì§€ feature\n",
        "        feats = model.encode_image(imgs)\n",
        "        feats = feats / feats.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        logits = classifier(feats)  # [B, num_classes]\n",
        "        preds = logits.argmax(dim=1).cpu().tolist()\n",
        "\n",
        "        # batch ì•ˆì˜ ê° ì´ë¯¸ì§€ì— ëŒ€í•´ filename + pred_label ì €ì¥\n",
        "        # DataLoaderëŠ” dataset.__getitem__ ìˆœì„œë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ë¯€ë¡œ,\n",
        "        # indexë¥¼ ë”°ë¡œ ê´€ë¦¬í•  í•„ìš” ì—†ì´ dataset.samplesì—ì„œ ê²½ë¡œë¥¼ êº¼ë‚¼ ìˆ˜ ìˆì–´.\n",
        "        # ë‹¤ë§Œ, ì—¬ê¸°ì„œëŠ” ì¡°ê¸ˆ ì‰½ê²Œ í•˜ê¸° ìœ„í•´, ë°˜ë³µë¬¸ ì™¸ë¶€ì—ì„œ ì „ì²´ íŒŒì¼ ë¦¬ìŠ¤íŠ¸ë¥¼ ê°™ì´ ëŒë¦¬ì.\n",
        "        # â†’ ì•„ë˜ì—ì„œ ë³„ë„ë¡œ ì²˜ë¦¬í• ê²Œìš”."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNL96sfZZoii",
        "outputId": "646a015a-85eb-43fb-ad30-7c57c642bd1d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Predicting all images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:18<00:00,  2.69s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rows = []\n",
        "model.eval()\n",
        "classifier.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for idx in tqdm(range(len(full_dataset)), desc=\"Predicting\"):\n",
        "        img, _ = full_dataset[idx]\n",
        "        img = img.to(device).unsqueeze(0)  # [1, C, H, W]\n",
        "\n",
        "        feat = model.encode_image(img)\n",
        "        feat = feat / feat.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        logits = classifier(feat)  # [1, num_classes]\n",
        "        pred_idx = int(logits.argmax(dim=1).item())\n",
        "        pred_label = full_dataset.classes[pred_idx]\n",
        "\n",
        "        # íŒŒì¼ ê²½ë¡œ/ì´ë¦„\n",
        "        img_path, _ = full_dataset.samples[idx]\n",
        "        filename = os.path.basename(img_path)\n",
        "\n",
        "        rows.append({\"filename\": filename, \"prediction\": pred_label})\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "save_path = \"/content/clip_linear_result.csv\"\n",
        "df.to_csv(save_path, index=False)\n",
        "print(\"ì˜ˆì¸¡ ê²°ê³¼ CSV ì €ì¥ ì™„ë£Œ:\", save_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4NHxavDPZu_W",
        "outputId": "96795bac-5e32-436e-be24-1e3d2bd1b2fc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Predicting: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:26<00:00,  3.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì˜ˆì¸¡ ê²°ê³¼ CSV ì €ì¥ ì™„ë£Œ: /content/clip_linear_result.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "\n",
        "# val_dataset: Subset(full_dataset, val_indices)\n",
        "# classifier, model, device, full_dataset.classes ê°€ ì´ë¯¸ ì •ì˜ë˜ì–´ ìˆë‹¤ê³  ê°€ì •\n",
        "\n",
        "num_classes = len(full_dataset.classes)\n",
        "class_names = full_dataset.classes\n",
        "\n",
        "# ì „ì²´ confusion matrixì™€ í´ë˜ìŠ¤ë³„ ì¹´ìš´íŠ¸\n",
        "conf_mat = np.zeros((num_classes, num_classes), dtype=int)\n",
        "val_counts = Counter()\n",
        "val_correct_counts = Counter()\n",
        "\n",
        "model.eval()\n",
        "classifier.eval()\n",
        "\n",
        "for i in tqdm(range(len(val_dataset)), desc=\"Val ë¶„ì„\"):\n",
        "    img, true_label = val_dataset[i]  # img: tensor, true_label: int\n",
        "    img = img.to(device).unsqueeze(0) # [1, C, H, W]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        feat = model.encode_image(img)\n",
        "        feat = feat / feat.norm(dim=-1, keepdim=True)\n",
        "        logits = classifier(feat)\n",
        "        pred_label = int(logits.argmax(dim=1).item())\n",
        "\n",
        "    conf_mat[true_label, pred_label] += 1\n",
        "    val_counts[true_label] += 1\n",
        "    if pred_label == true_label:\n",
        "        val_correct_counts[true_label] += 1\n",
        "\n",
        "print(\"\\n[í´ë˜ìŠ¤ë³„ ê²€ì¦ ì •í™•ë„]\")\n",
        "for cls_idx, cls_name in enumerate(class_names):\n",
        "    total = val_counts[cls_idx]\n",
        "    correct = val_correct_counts[cls_idx]\n",
        "    acc = (correct / total * 100) if total > 0 else 0.0\n",
        "    print(f\"{cls_name:12s} : {correct:2d} / {total:2d}  ({acc:.1f}%)\")\n",
        "\n",
        "print(\"\\n[Confusion Matrix] (í–‰=ì‹¤ì œ, ì—´=ì˜ˆì¸¡)\")\n",
        "print(\"     \" + \"  \".join([f\"{name[:4]:>4s}\" for name in class_names]))\n",
        "for i, row in enumerate(conf_mat):\n",
        "    row_str = \"  \".join([f\"{v:4d}\" for v in row])\n",
        "    print(f\"{class_names[i][:4]:>4s}  {row_str}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W67A_uUGbKcD",
        "outputId": "13f403a1-3629-45f3-8bc7-32c3076c5f53"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Val ë¶„ì„: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:07<00:00,  2.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[í´ë˜ìŠ¤ë³„ ê²€ì¦ ì •í™•ë„]\n",
            "front        :  2 /  2  (100.0%)\n",
            "front_left   :  0 /  4  (0.0%)\n",
            "front_right  :  1 /  3  (33.3%)\n",
            "inside       :  2 /  2  (100.0%)\n",
            "left         :  0 /  0  (0.0%)\n",
            "rear         :  1 /  1  (100.0%)\n",
            "rear_left    :  1 /  2  (50.0%)\n",
            "rear_right   :  1 /  1  (100.0%)\n",
            "right        :  1 /  3  (33.3%)\n",
            "tire         :  2 /  2  (100.0%)\n",
            "\n",
            "[Confusion Matrix] (í–‰=ì‹¤ì œ, ì—´=ì˜ˆì¸¡)\n",
            "     fron  fron  fron  insi  left  rear  rear  rear  righ  tire\n",
            "fron     2     0     0     0     0     0     0     0     0     0\n",
            "fron     0     0     1     0     0     0     0     2     1     0\n",
            "fron     1     0     1     0     0     0     0     1     0     0\n",
            "insi     0     0     0     2     0     0     0     0     0     0\n",
            "left     0     0     0     0     0     0     0     0     0     0\n",
            "rear     0     0     0     0     0     1     0     0     0     0\n",
            "rear     0     0     0     0     0     0     1     1     0     0\n",
            "rear     0     0     0     0     0     0     0     1     0     0\n",
            "righ     0     0     0     0     2     0     0     0     1     0\n",
            "tire     0     0     0     0     0     0     0     0     0     2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torchvision.models import vit_b_16, ViT_B_16_Weights\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"ì‚¬ìš© ë””ë°”ì´ìŠ¤:\", device)\n",
        "\n",
        "# 1) ê°™ì€ split ì‚¬ìš©: train_indices, val_indices\n",
        "weights = ViT_B_16_Weights.IMAGENET1K_V1\n",
        "vit_transform = weights.transforms()  # ViT ê¶Œì¥ ì „ì²˜ë¦¬\n",
        "\n",
        "vit_full_dataset = datasets.ImageFolder(data_dir, transform=vit_transform)\n",
        "\n",
        "vit_train_dataset = Subset(vit_full_dataset, train_indices)\n",
        "vit_val_dataset   = Subset(vit_full_dataset, val_indices)\n",
        "\n",
        "vit_train_loader = DataLoader(vit_train_dataset, batch_size=16, shuffle=True)\n",
        "vit_val_loader   = DataLoader(vit_val_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "num_classes = len(vit_full_dataset.classes)\n",
        "print(\"í´ë˜ìŠ¤ ìˆ˜:\", num_classes, \"â†’\", vit_full_dataset.classes)\n",
        "\n",
        "# 2) ViT ëª¨ë¸ ì •ì˜\n",
        "vit_model = vit_b_16(weights=weights)\n",
        "vit_model.heads.head = nn.Linear(vit_model.heads.head.in_features, num_classes)\n",
        "vit_model = vit_model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(vit_model.parameters(), lr=1e-4, weight_decay=1e-3)\n",
        "\n",
        "def train_vit_one_epoch(model, loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for imgs, labels in tqdm(loader, desc=\"ViT Train\"):\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(imgs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * imgs.size(0)\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += imgs.size(0)\n",
        "\n",
        "    return total_loss / total, correct / total * 100.0\n",
        "\n",
        "def eval_vit(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in tqdm(loader, desc=\"ViT Val\"):\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            outputs = model(imgs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            total_loss += loss.item() * imgs.size(0)\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += imgs.size(0)\n",
        "\n",
        "    return total_loss / total, correct / total * 100.0\n",
        "\n",
        "EPOCHS = 10\n",
        "best_vit_val_acc = 0.0\n",
        "best_vit_state = None\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    train_loss, train_acc = train_vit_one_epoch(vit_model, vit_train_loader, optimizer, criterion, device)\n",
        "    val_loss, val_acc = eval_vit(vit_model, vit_val_loader, criterion, device)\n",
        "\n",
        "    print(f\"[ViT Epoch {epoch:02d}] \"\n",
        "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% | \"\n",
        "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "    if val_acc > best_vit_val_acc:\n",
        "        best_vit_val_acc = val_acc\n",
        "        best_vit_state = vit_model.state_dict()\n",
        "\n",
        "print(\"\\nViT ìµœê³  ê²€ì¦ ì •í™•ë„:\", best_vit_val_acc, \"%\")\n",
        "\n",
        "if best_vit_state is not None:\n",
        "    vit_model.load_state_dict(best_vit_state)\n",
        "    print(\"<Best ViT weights loaded>\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nt7uDVMTbR3G",
        "outputId": "f61609c6-66e6-4e56-c737-8b0953c92259"
      },
      "execution_count": 12,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ì‚¬ìš© ë””ë°”ì´ìŠ¤: cpu\n",
            "í´ë˜ìŠ¤ ìˆ˜: 10 â†’ ['front', 'front_left', 'front_right', 'inside', 'left', 'rear', 'rear_left', 'rear_right', 'right', 'tire']\n",
            "Downloading: \"https://download.pytorch.org/models/vit_b_16-c867db91.pth\" to /root/.cache/torch/hub/checkpoints/vit_b_16-c867db91.pth\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 330M/330M [00:06<00:00, 51.8MB/s]\n",
            "ViT Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [02:43<00:00, 32.63s/it]\n",
            "ViT Val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:12<00:00,  6.30s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ViT Epoch 01] Train Loss: 2.2022, Train Acc: 18.75% | Val Loss: 1.9705, Val Acc: 30.00%\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ViT Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [02:36<00:00, 31.33s/it]\n",
            "ViT Val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:12<00:00,  6.29s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ViT Epoch 02] Train Loss: 0.9988, Train Acc: 82.50% | Val Loss: 1.1904, Val Acc: 55.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ViT Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [02:34<00:00, 30.93s/it]\n",
            "ViT Val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:12<00:00,  6.30s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ViT Epoch 03] Train Loss: 0.2733, Train Acc: 100.00% | Val Loss: 1.0671, Val Acc: 65.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ViT Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [02:34<00:00, 30.83s/it]\n",
            "ViT Val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:12<00:00,  6.30s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ViT Epoch 04] Train Loss: 0.0799, Train Acc: 100.00% | Val Loss: 1.2105, Val Acc: 55.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ViT Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [02:37<00:00, 31.45s/it]\n",
            "ViT Val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:12<00:00,  6.17s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ViT Epoch 05] Train Loss: 0.0378, Train Acc: 100.00% | Val Loss: 0.9258, Val Acc: 70.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ViT Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [02:37<00:00, 31.46s/it]\n",
            "ViT Val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:12<00:00,  6.26s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ViT Epoch 06] Train Loss: 0.0175, Train Acc: 100.00% | Val Loss: 0.8832, Val Acc: 65.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ViT Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [02:33<00:00, 30.72s/it]\n",
            "ViT Val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:12<00:00,  6.16s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ViT Epoch 07] Train Loss: 0.0112, Train Acc: 100.00% | Val Loss: 0.8598, Val Acc: 70.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ViT Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [02:33<00:00, 30.76s/it]\n",
            "ViT Val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:12<00:00,  6.24s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ViT Epoch 08] Train Loss: 0.0071, Train Acc: 100.00% | Val Loss: 0.8634, Val Acc: 70.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ViT Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [02:32<00:00, 30.51s/it]\n",
            "ViT Val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:12<00:00,  6.17s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ViT Epoch 09] Train Loss: 0.0053, Train Acc: 100.00% | Val Loss: 0.8964, Val Acc: 70.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ViT Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [02:32<00:00, 30.55s/it]\n",
            "ViT Val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:12<00:00,  6.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ViT Epoch 10] Train Loss: 0.0043, Train Acc: 100.00% | Val Loss: 0.9207, Val Acc: 65.00%\n",
            "\n",
            "ViT ìµœê³  ê²€ì¦ ì •í™•ë„: 70.0 %\n",
            "<Best ViT weights loaded>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}